# -*- coding: utf-8 -*-
"""hw1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JbfKwFHyH0zBqNf4NpH56oq7dQQYby65
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt

import json

!pip install nltk
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize

import matplotlib.pyplot as plt

class HW1:
  def __init__(self):
    print("................")
  #set file input path
  def setInputPath(self, inputPath):
    self.input_path = inputPath

  #set set output file path
  #use to store and hold the data.txt file
  def setOutputPath(self, outputPath):
    self.output_path = outputPath

  #read the file and extract the messages part
  #store the extracted message in data.txt
  def read_json_file_and_extract_message(self):
    messages = []

    with open(self.input_path , 'r') as input_file:
      for line in input_file:
        #json the content
        data = json.loads(line)
        #extract messages
        if "messages" in data:
          messages.extend(data["messages"])

    #store the content in the data.txt
    with open(self.output_path, 'w') as output_file:
      for message in messages:
        output_file.write(message +'\n')

  #an helper method to split
  #the line into sentences
  def split_method_to_count(self, sentence):
    num_of_words = 0
    #loop through sentence and use python split to turn into array
    #get the length of the array. which is the number
    #of the words
    for sent in sentence:
      words_split = sent.split(' ')
      num_of_words += len(words_split)

    #return the total number of tokens
    return num_of_words

  #an help method to split
  #uses nltk word tokenize
  def tokenize_nltk_to_count(self, sentence):
    num_of_words = 0
    #loop through the sentence
    #for each sentence
    #tokenize and get the length
    for sent in sentence:
      sent_tokenized = word_tokenize(sent)
      num_of_words += len(sent_tokenized)

    #return the total number of tokens
    return num_of_words

  def split_using_sent_tokenize_then_use_python_split_to_split_the_sentences_and_nltk_tokenize(self):
      #holds the total number of sentences
      self.num_of_sentences = 0
      #holds the total number of token using python split
      self.num_of_words_with_python_split = 0
      #holds the total number of token using nltk word tokenize split
      self.num_of_words_with_nltk_tokenize = 0

      #loop through the data.txt file
      with open(self.output_path, 'r') as output_file:
        for line in output_file:
          #tokenize the sentence
          sentences = sent_tokenize(line)
          #check if token is empty
          #if empty, do not bother
          if len(sentences) ==0:
            continue
          if '' in sentences:
            continue
          if "" in sentences:
            continue

          if sentences:
            #add number of sentences
            self.num_of_sentences += len(sentences)
            #split the sentences using python split to count
            self.num_of_words_with_python_split += self.split_method_to_count(sentences)
            #using the the nltk to count
            self.num_of_words_with_nltk_tokenize += self.tokenize_nltk_to_count(sentences)


      print("The total number of sentences are: ", self.num_of_sentences)

      print("The total number of tokens using python split: ", self.num_of_words_with_python_split)

      print("The total number of tokens using nltk tokenize: ", self.num_of_words_with_nltk_tokenize)


  def count_and_lower_case_using_split(self):
    lowercased_words = []
    with open(self.output_path, 'r') as output_file:
      for line in output_file:
        words =  line.split()
        if len(words) == 0:
          continue

        lowercased_words.extend(word.lower() for word in words)


    print("the total number of tokens in lowercased words", len(lowercased_words))
    print("the total number of types of tokens in lowercased words", len(set(lowercased_words)))

  #lowercase all the words
  #save the new lowercased file in a new file called lowercased.txt
  def lower_case_all_the_words(self, lowercasePath):
    self.lowercased_file_path_of_data_txt = lowercasePath
    with open(self.output_path, 'r') as output_file:

      file_content = output_file.read()

      #convert to lower case
      self.lowercase_file = file_content.lower()

      with open(self.lowercased_file_path_of_data_txt, 'w') as lower_path_file:
        lower_path_file.write(self.lowercase_file)

  #count token types from the lowercased.txt
  def count_tokens_and_types_after_lowercasing(self):
    #set to count token
    #it avoids duplicates
    types = set()
    #holds total token
    total_token = 0
    #to track the number of duplicates
    duplicates = 0
    #a dictionary to count token frequencies
    lower_dict = {}
    with open(self.lowercased_file_path_of_data_txt, 'r') as low_path_file:
      for line in low_path_file:
        #tokenize the line into sentences
        line_sent_tokenize = sent_tokenize(line)
        # print(line_sent_tokenize, len(line_sent_tokenize))

        if len(line_sent_tokenize) == 0:
          continue

        #loop through the tokenization i.e each sentences
        #loop through each tokenize sentences
        for sents in line_sent_tokenize:
          sents_to_word_tokenize = word_tokenize(sents)
          # loop through each tokens
          for tk in sents_to_word_tokenize:
            # add token to the set
            types.add(tk)
            #count duplicates
            if tk in types:
              duplicates += 1
            #track the frequency
            #with the lower_dict dictionary
            if tk in lower_dict:
              lower_dict[tk] += 1
            else:
              lower_dict[tk] = 1

          # print(sents_to_word_tokenize, len(sents_to_word_tokenize))
          total_token += len(sents_to_word_tokenize)


    print("the total number of tokens in lowercased words using nltk package",
          total_token)
    print("the total number of types in lowercased words nltk package",
          len(types))
    # print("the total number of types in lowercased words using dictionary", len(lower_dict))
    # # print("found this much duplicates", duplicates)
    summed_up_frequency = sum(list(lower_dict.values()))
    # print("there are total number of tokens using dictionary to count", summed_up_frequency)

    sorted_dict = dict(sorted(lower_dict.items(), key=lambda item: item[1], reverse =True))

    word_type = next(iter(sorted_dict))
    print("the most freqent word type in lowercased version is: ", word_type)

    counter = 0
    for key, value in sorted_dict.items():
      counter += 1

      if counter > 4:
        print("the fifth most freqent word lowercased version is: ",key)
        break

  #count the types and frequency
  #from the not pre lowercased file
  def get_words_types_and_frequency(self):
    #make a dictionary
    word_dict = {}
    with open(self.output_path, 'r') as output_file:
      for line in output_file:
        tokenized_word = word_tokenize(line)
        for word in tokenized_word:
          if word in word_dict:
            word_dict[word] += 1
          else:
            word_dict[word] = 1


    sorted_dict = dict(sorted(word_dict.items(), key=lambda item: item[1], reverse =True))

    word_type = next(iter(sorted_dict))
    print("the most freqent word in data.txt (pre lowercasing) using nltk package is: ",
          word_type)

    counter = 0
    for key, value in sorted_dict.items():
      counter += 1

      if counter > 4:
        print("the fifth most freqent word in data.txt (pre lowercasing) using nltk package is: ",
              key)
        break

    ranked_words = list(word_dict.keys())
    frequencies = list(word_dict.values())


    # Plotting the graph
    plt.figure(figsize=(10, 6))
    plt.semilogy(range(1, len(ranked_words) + 1), frequencies, marker='o', linestyle='-', color='b')
    # plt.xscale('log')  # X-axis on a logarithmic scale
    # plt.yscale('log')  # Y-axis on a logarithmic scale
    plt.title("Zipf's Law Graph")
    plt.xlabel("Ranked Words (Log Scale)")
    plt.ylabel("Frequencies (Log Scale)")
    plt.grid(True)
    plt.show()


input_ile_path = '/content/drive/MyDrive/nlp/train.jsonl'
output_file_path = '/content/drive/MyDrive/nlp/data.txt'

lowercased_file_path = '/content/drive/MyDrive/nlp/lowercased.txt'


homework_one = HW1()

#set input path
homework_one.setInputPath(input_ile_path)

#set output path
homework_one.setOutputPath(output_file_path)

#read file and extract [messages]
homework_one.read_json_file_and_extract_message()

#question a and b and c
homework_one.split_using_sent_tokenize_then_use_python_split_to_split_the_sentences_and_nltk_tokenize()

# #using split function to lower case and count tokens and types
# homework_one.count_and_lower_case_using_split()


#lowercased content
homework_one.lower_case_all_the_words(lowercased_file_path)

#part D
homework_one.count_tokens_and_types_after_lowercasing()

#part f and g
homework_one.get_words_types_and_frequency()