# -*- coding: utf-8 -*-
"""hw2_nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TItXTtsHULEAaJuTmtjftwCZCxE2Uju-
"""

#Emmanuel Adebayo
#NLP N gram model assignment



#download and install the necessary files

!pip install nltk
import nltk
nltk.download('brown')
from nltk.corpus import brown

import numpy as np

from collections import Counter


from nltk.util import bigrams, ngrams
import re

#heler function to temove punctuation
def remove_punctuation(tokens):
    # Define punctuation characters
    # punctuation = r'[,.!?;:\(\)\[\]\{\}\-–—\'"‘’“”…`]'
    punctuation = r'[\W_]'

    # Remove punctuation from all tokens and filter out empty strings
    modified_tokens = [re.sub(punctuation, '', token) for token in tokens if re.sub(punctuation, '', token)]

    return modified_tokens


# Lowercase all the words and remove punctuation
def convert_to_lowercase(data):
    lowercase_data = []
    for sentence in data:
        # Convert each word to lowercase and remove punctuation
        lowercase_sentence = remove_punctuation([word.lower() for word in sentence])
        lowercase_data.append(lowercase_sentence)

    return lowercase_data



#helper function to dot punctuation
#add a token that signifies the beginning and end of the line
def remove_sentence_that_ends_dot(data):
    modified_data = [sentence.copy() for sentence in data]

    for sentence in modified_data:
        # if sentence[-1] == '.':
        begin_token = '<s>'
        end_token = '</s>'
        sentence.insert(0, begin_token)
        sentence.append(end_token)
        # sentence[-1] = end_token

    return modified_data

#add a token that signifies the beginning and end of the line to every sentence
def add_begin_token_and_end_token(data):
    modified_data = [sentence.copy() for sentence in data]

    for sentence in modified_data:
        if '</s>' not in sentence and '<s>' not in sentence:
            begin_token = '<s>'
            end_token = '</s>'
            sentence.insert(0, begin_token)

            sentence.append(end_token)


    return modified_data

def flatten_extend(matrix):
    flat_list = []
    for row in matrix:
        flat_list.extend(row)
    return flat_list

#count non zero ngram
#the model can takes unigram ad bigram as input
def count_non_zero_ngrams(model):
  non_zero_count = 0
  for ngram, count in model.items():
    # print(value == 0)
    if count != 0:
      non_zero_count += 1
  return non_zero_count

#print the top most count ngram
#it ca take unigram  as input
def most_common_unigrams_and_their_probability(model, len_of_corpus, data_dsitribution):
  top_ten = sorted(model, key= model.get, reverse=True)[:10]

  print(f"10 most common unigrams (in terms of counts) from {data_dsitribution} dataset with their probabilities P (wt) (using MLE).")

  print("index -", "unigram -", "frequency -", "probability")
  index = 1
  for key in top_ten:
    print(index,key, model[key], model[key] / len_of_corpus)
    index += 1

#calculate the probability of the top ten ngram
#it can take bigram as input
def most_common_bigrams_and_their_probability(unigram_model_for_estimation, bigram_model_for_estimation, data_dsitribution):
  top_ten = sorted(bigram_model_for_estimation, key= bigram_model_for_estimation.get, reverse=True)[:10]
  print(f"10 most common bigrams (in terms of counts) from {data_dsitribution} dataset with their probabilities P (wt) (using MLE).")

  print("index -", "bigram- ", "frequency- ", "probability")
  index = 1
  for key in top_ten:

    bigram_count = bigram_model_for_estimation[key]
    token_n_1 = key[0]
    token_n_1_count = unigram_model_for_estimation[token_n_1]

    probability = bigram_count / token_n_1_count

    print(index, key, bigram_count,  probability)
    index += 1


#calculatr thr probability of a given sentence
def Probability_of_a_given_sentence(sentence, unigram_model_for_estimation, bigram_model_for_estimation, data_dsitribution):
  list_of_bigrams = list( ngrams(sentence, 2))

  total_probability = 1
  for bigram in list_of_bigrams:
    # print(bigram, bigram_model_for_estimation[bigram])

    token_n_1 = bigram[0]
    bigram_count = bigram_model_for_estimation[bigram]
    token_n_1_count = unigram_model_for_estimation[token_n_1]

    if token_n_1_count == 0:
      total_probability = 0
      break
    prob_i = bigram_count / token_n_1_count
    total_probability = total_probability * prob_i

    # print(prob_i)

  print(f"the probability of {sentence} in {data_dsitribution} is", total_probability)


#add laplace add one smoothing
def Probability_of_a_given_sentence_with_lapace_add_one_smothing(sentence, len_of_corpus, unigram_model_for_estimation, bigram_model_for_estimation, data_dsitribution):
  list_of_bigrams = list( ngrams(sentence, 2))

  total_probability = 1
  for bigram in list_of_bigrams:
    # print(bigram, bigram_model_for_estimation[bigram])

    token_n_1 = bigram[0]
    bigram_count = bigram_model_for_estimation[bigram] + 1
    token_n_1_count = unigram_model_for_estimation[token_n_1] + len_of_corpus

    if token_n_1_count == 0:
      total_probability = 0
      break
    prob_i = bigram_count / token_n_1_count
    total_probability = total_probability * prob_i



  print(f"the probability of {sentence} with laplace add-one smoothing in {data_dsitribution} is", total_probability)



def main():
  #get data
  news_data = brown.sents(categories = 'news')
  romance_data = brown.sents(categories = 'romance')

  #lowercase all the words
  lowercased_news_data = convert_to_lowercase(news_data)
  lowercased_romance_data = convert_to_lowercase(romance_data)

  #emove tokens that only consist of punctuation.
  #For example, change [’Phil’, ’nodded’, ’.’]
  # into [’Phil’, ’nodded’] before adding <s> and </s>. Therefore, [’Phil’, ’nodded’, ’.’] will become [’<s>’,
  # ’Phil’, ’nodded’, ’</s>’]
  news_data_period_modification = remove_sentence_that_ends_dot(lowercased_news_data)
  romance_data_period_modification = remove_sentence_that_ends_dot(lowercased_romance_data)

  #include <s> and </s> before and after each sentence.
  add_first_and_last_token_news_data = add_begin_token_and_end_token(news_data_period_modification)
  add_first_and_last_token_romance_data = add_begin_token_and_end_token(romance_data_period_modification)

  flattened_news_data = flatten_extend(add_first_and_last_token_news_data)
  flattened_romance_data = flatten_extend(add_first_and_last_token_romance_data)



  #returns a dictionary of tokens and their frequency

  #for news data
  unigram_model_news_data = Counter(flattened_news_data)
  bigram_model_news_data = Counter(bigrams(flattened_news_data))

  #for romance data
  unigram_model_romance_data = Counter(flattened_romance_data)
  bigram_model_romance_data = Counter(bigrams(flattened_romance_data))

  #question a
  print("question a")
  print("The total number of non zero unigram for the news_data: ", count_non_zero_ngrams(unigram_model_news_data))
  print("The total number of non zero unigram for the romance_data: ",count_non_zero_ngrams(unigram_model_romance_data))
  print("\n")

  # #question b
  print("question b")
  print("The total number of non zero bigram for the news_data: ",count_non_zero_ngrams(bigram_model_news_data))
  print("The total number of non zero bigram for the romance_data: ",count_non_zero_ngrams(bigram_model_romance_data))
  print("\n")

  # #question c
  #news_data
  print("question c")
  most_common_unigrams_and_their_probability(unigram_model_news_data, len(flattened_news_data), "news_data")
  #romance data
  most_common_unigrams_and_their_probability(unigram_model_romance_data, len(flattened_romance_data), "romance_data")
  print("\n")

  #question d
  # news_data
  print("question d")
  most_common_bigrams_and_their_probability(unigram_model_news_data, bigram_model_news_data, "news_data")
  #romance data
  most_common_bigrams_and_their_probability(unigram_model_romance_data, bigram_model_romance_data, "romace_data")
  print("\n")

  # #question e
  print("question e")
  sentence = ['<s>', 'I', 'loved', 'her', 'when', 'she', 'laughed', '</s>']
  #lowercase senetence
  sentence_lower = [word.lower() for word in sentence]
  # print(sentence_lower)
  Probability_of_a_given_sentence(sentence_lower, unigram_model_news_data, bigram_model_news_data, "news_data")
  print("\n")

  # #question f
  print("question f")
  Probability_of_a_given_sentence(sentence_lower, unigram_model_romance_data, bigram_model_romance_data, "romace data")
  print("\n")

  # #question g
  print("question g")
  # len( set(flattened_news_data) )
  # len( set(flattened_romance_data) )
  Probability_of_a_given_sentence_with_lapace_add_one_smothing(sentence_lower, len( unigram_model_news_data ), unigram_model_news_data, bigram_model_news_data, "news_data")
  Probability_of_a_given_sentence_with_lapace_add_one_smothing(sentence_lower, len( unigram_model_romance_data ), unigram_model_romance_data, bigram_model_romance_data, "romace data")


if __name__ == "__main__":
  main()